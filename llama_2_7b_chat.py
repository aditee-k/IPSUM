# -*- coding: utf-8 -*-
"""Llama-2-7b-chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GrqpVfd4BhUI0y-UoA5awogK54kRkacj
"""

!huggingface-cli login
# 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
#Shaswata is editing

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

# The model that you want to train from the Hugging Face hub
model_name="meta-llama/Llama-2-7b-chat-hf"

# The instruction dataset to use
dataset_name = "Mangacoder007/SE-Dataset"

new_model="Llama-2-7b-chat-finetune"

# dataset_name = "mlabonne/guanaco-llama2-1k"
# dataset_name="/content/drive/MyDrive/Minor Project  Data Collection  - Total (1).csv"

# Fine-tuned model name
new_model="Llama-2-7b-chat-finetune"

# QLoRA parameters

# LoRA attention dimension
lora_r=64
# Alpha parameter for LoRA scaling
lora_alpha=16
# Dropout probability for LoRA layers
lora_dropout=0.1


# bitsandbytes parameters

# Activate 4-bit precision base model loading
use_4bit=True
bnb_4bit_compute_dtype="float16"
bnb_4bit_quant_type="nf4"
use_nested_quant=False


# TrainingArguments parameters

# Output directory where the model predictions and checkpoints will be stored
output_dir="./results"
# Number of training epochs
num_train_epochs=1
# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16=False
bf16=False
# Batch size per GPU for training
per_device_train_batch_size=4
# Batch size per GPU for evaluation
per_device_eval_batch_size=4
# Number of update steps to accumulate the gradients for
gradient_accumulation_steps=1
# Enable gradient checkpointing
gradient_checkpointing=True
# Maximum gradient normal (gradient clipping)
max_grad_norm=0.3
# Initial learning rate (AdamW optimizer)
learning_rate=2e-4
# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay=0.001
# Optimizer to use
optim="paged_adamw_32bit"
# Learning rate schedule
lr_scheduler_type="cosine"
# Number of training steps (overrides num_train_epochs)
max_steps=-1
# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio=0.03
# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length=True
# Save checkpoint every X updates steps
save_steps=0
# Log every X updates steps
logging_steps=25


# SFT parameters

# Maximum sequence length to use
max_seq_length=None
# Pack multiple short examples in the same input sequence to increase efficiency
packing=False
# Load the entire model on the GPU 0
device_map={"":0}

# Load dataset (you can process it here)
dataset=load_dataset(dataset_name, split="train")

# Load tokenizer and model with QLoRA configuration
compute_dtype=getattr(torch, bnb_4bit_compute_dtype)


bnb_config=BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

print(dataset)
for i in range(30):
    print(dataset[i])

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
  major, _ = torch.cuda.get_device_capability()
  if major >= 8:
    print("=" * 80)
    print("Your GPU supports bfloat16: accelerate training with bf16=True")
    print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache=False
model.config.pretraining_tp=1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM"
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer= SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field='text',
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir results/runs

## CAlCULATE THE TDI
def calc_TDI():
    TDI=14
    influence=input("Enter the degree of influenceof the func [incidental, moderate, average, significant, essential, n/a]: ")
    influence=influence.lower()
    degree_of_influence={"n/a":0, "na":0, "incidental":1, "moderate":2, "average":3, "significant":4, "essential":5}

    if influence in degree_of_influence:
        TDI = TDI * degree_of_influence[influence]

    return TDI

def calc_UFP():
    attribute= [[3, 4, 6], [4, 5, 7], [3, 4, 6], [7, 10, 15], [5, 7, 10]]
    complexity_score={"simple":0, "average": 1, "complex": 2}
    f_type_score={"input": 0,"output":1, "inquiry":2, "internal logical file": 3, "ilf":3, "external logical file": 4, "elf":4}
    data=[]
    UFP=0

    while True:
        file_type=input("Enter file type: [input, output, internal logical file (ilf), external logical file(elf), inquiry]")
        n_files=int(input("Enter number of files:"))
        complexity=input("Enter its complexity: [simple, average, complex]: ")

        file_type=file_type.lower()
        complexity=complexity.lower()

        data.append({
            "file_type": file_type,
            "complexity": complexity,
            "n_file": n_files
        })

        choice=input("Want to enter more files? (y/n):")
        choice=choice.lower()
        if choice=="n":
            break

    for file in data:
        print(f"{file['n_file']} {file['file_type']} File that are {file['complexity']}")

    print("\n")
    length=len(data)
    for i in range (length):
        file_type = data[i]['file_type']
        complexity_type=data[i]['complexity']

        # print(f"file_type: {file_type}")
        # print(f"complexity_type: {complexity_type}")

        if file_type in f_type_score:
            f_type=f_type_score[file_type]
            # print(f"f_type: {f_type}")

        if complexity_type in complexity_score:
            c_score=complexity_score[complexity_type]
            # print(f"c_score: {c_score}")

        UFP = UFP + (attribute[f_type][c_score] * data[i]['n_file'])
        # print(UFP)

    # print(f"Total UFP: {UFP}")
    return UFP

def FP():
    TDI=calc_TDI()
    CAF=0.65+0.01*TDI
    UFP=calc_UFP()
    FP=UFP*CAF
    print(f"The function point is: {FP}")

def contains_cocomo_keywords(text):
  keywords = ["cocomo", "COCOMO", "semi-detached", "organic", "embedded", "KLOC"]
  for keyword in keywords:
    if keyword.lower() in text.lower():
      return True
  return False

def check_function_point_analysis(text):
    text=text.lower()
    keywords=["function point"]
    for keyword in keywords:
        if keyword in text:
            return True
    return False

def cocomo():
  kloc = float(input("Enter the lines of code in thousands: "))
  var = input("Enter a variant (organic, semi detached, embedded): ").lower()  #Convert input to lowercase

  valid = ["organic", "semi detached", "embedded"]

  if var in valid:
    if var == "organic":
      effort = 2.4*(kloc ** 1.05)
      devtime = 2.5 * (effort ** 0.38)
    elif var == "semi detached":
      effort = 3*(kloc ** 1.12)
      devtime = 2.5 * (effort ** 0.35)
    else:
      effort = 3.6*(kloc ** 1.20)
      devtime = 2.5 * (effort ** 0.32)
    wfr = effort/devtime
    prod = kloc/effort
    print(f"Effort: {effort}, \nDev. time: {devtime}, \nWork force requirements (WFR): {wfr}, \nProductivity: {prod}")

  else:
    print(var, "is not a valid option.")

# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our next model
#prompt="What are some common design patterns used in system design?"
prompt = input("Enter your query.")
if contains_cocomo_keywords(prompt):
  cocomo()
elif check_function_point_analysis(prompt):
    FP()
else:
  pipe=pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=150)
  result=pipe(f"<s>[INST] {prompt} [/INST]")
  # print(result[0]['generated_text'])
  second_inst_index = result[0]['generated_text'].find('[INST]', result[0]['generated_text'].find('[INST]') + 1)
  #Extract the portion of text before the second '[INST]'
  final_output = result[0]['generated_text'][:second_inst_index].strip()
  print(final_output)

# DO NOT RUN AFTER THIS

# Empty VRAM
del model
del pipe
del trainer
import gc
gc.collect()
gc.collect()

# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!huggingface-cli login

model.push_to_hub("Mangacoder007/Llama-2-7b-chat-finetune", check_pr=True)

tokenizer.push_to_hub("Mangacoder007/Llama-2-7b-chat-finetune",check_pr=True)